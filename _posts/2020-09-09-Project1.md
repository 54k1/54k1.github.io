---
layout: post
title: "Project #1: Buffer Pool Manager"
comments: true
description: ""
keywords: ""
---
## Premise

The assumption is that the primary storage location of data is a non-volatile spinning disk (Hard disk). Any data that is read is first fetched into the volatile DRAM from the disk.

The DRAM also has a capacity much less than the spinning disk hard drive. Therefore, we need to evict data from the DRAM periodically to make way for new data to come into the DRAM.

The access times of a HDD and DRAM memory differ greatly: 10,000,000 ns to 100 ns. In order to ensure fast accesses of data, we might have to even prefetch some data based on some hueristics to blur the access time difference.

The goal is to provide the illusion of an large memory (of a disk) and fast access time (of a DRAM). This goal is achieved by being smart about what you bring into the memory (DRAM) and when.

Another assumption is that the database system would be an active process running on an operating system.
Which means we can't circumvent the file system. We must finally store our data as files on disk. The file system also does some work such as journaling which the database would do anyway. There seems to be some overhead due to the file system. However the engineering cost of developing a custom file system and maintaining it is far greater.
A custom file system is also less portable.


## Database Storage
We finally store the data as files on disk.

### Data organization in files
Data in files is stored as a collection of pages. A page a fixed length block of data. Each page contains tuples, indexes, log records, etc.

The database page can differ in size from a hardware page. IDK why this is the case.
Possible answer: In case the tuple length is greater than page size, then we'll have too many overflow pages. Probably a pain to deal with them.

The pages themselves can be scattered anywhere in the file. This is called a heap file organization. An indirection layer (Page Directory) maps the pages to physical locations on the disk.


### Data organization in Pages
Each page contains a header to store metadata about the pages (free slots, etc), and a body to store the tuples.
The header might also contain the tuple layout (schema) for the tuples in the page. Other systems just store the tuple layout seperately to avoid the repitition.

Turns out it is useful to have a header for tuples as well which stores meta data related to concurrency control. Remember that these structures sit in the RAM as well and such information is critical of the DBMS to function properly.
Maybe we can store it in a seperate data structure in the memory and avoid the overhead to storing such operational meta data in the disk.

## Buffer Pool

### mmap
mmap is a system call which maps the contents of a file to the address space of a process in posix. Since the database finally stores data in files on disk, it is tempting to use something like mmap.
However there are some disadvantages of mmap:
- No control over what pages are dropped.
The OS is free to drop whatever pages it wants. However, the database system can make a better judgement of what pages to drop since it is more familiar with what kind of data is being handled (indexes, tuples, etc.).
- Little control over prefetching: The database system can do a better job at prefetching pages.
- Flusing dirty pages to the disk in the right order - ??

### Buffer Pool manager
The alternative to mmap is to implement a buffer pool manager which manages the database pages.
This lets us have our own replacement policy which hopefully would be better than what the OS can do.

The buffer pool manager can be a multithreaded buffer pool in order to facilitate multithreading of parts of queries to make it go fast.
We therefore require our buffer pool to be thread safe.

Apart from being thread safe, the buffer pool is quite simple. The API provided by the buffer pool facilitates retrieval, writing of pages through the disk manager. In case any page is to be replaced, the replacement policy (implemented as a replacer) will hint the buffer pool as to which page to discard.

### How do we expect the buffer pool to be read

The higher levels of the system would request for specific pages given a page id. The buffer pool is then expected to return a Page object with the data corresponding to the page with the given page\_id.
Note that the number of pages the buffer pool can hold in the memory is limited.
Therefore, the same memory allocated in the beginning must be used to 'host' pages.

#### Replacement
In case all pages are full (nothing in the free pages list), we need to evict some page. This job is delegated to the replacer.

#### Pinning and Unpinning
- When you pin a page, you tell the buffer pool to hold on to the page and not evict it.

- When you unpin a page, you tell the buffer pool that you don't require the page anymore and the buffer pool is free to evict the page.
The service which holds on to the page when it is pinned, may write to it which means it can be dirty when it's unpinned. Therefore, we need to write back those changes in case it is dirty. Just mark it as dirty and it will be written back when it is evicted.

#### New Page
When a new page is requested, first the disk manager is requested for a new page. The DM then gives the new page a frame (if possible). The new `Page*` is returned to the caller.

Note that when all pages are pinned, we cannot bring in any new pages into the buffer pool. We are basically stuck and can only start handling those requests once some page is unpinned. In this case, a `nullptr` is returned to the caller indicating that a new page cannot be allocated.